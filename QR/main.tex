\documentclass[12pt,letterpaper,onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[lmargin=71pt, tmargin=1.2in]{geometry} % For centering solution box

% \chead{\hline} % Uncomment to draw a line below the header
\thispagestyle{empty} % Remove header/footer from the first page

\begin{document}

\begingroup
    \centering
    \LARGE \textbf{Matrix Theory}\\
    \LARGE \textbf{Eigenvalue Computation}\\[0.5em]
    
    \large Kotha Pratheek Reddy\\
    \large ai24btech11019\par
\endgroup

\rule{\textwidth}{0.4pt}

\pointsdroppedatright % Self-explanatory
\printanswers
\renewcommand{\solutiontitle}{\noindent\textbf{Answer:}\enspace} % Customize solution box heading

\section{Algorithms}
The chosen algorithm is a combination of the \textbf{Householder transformation} and the \textbf{QR algorithm}, which computes all eigenvalues of a given matrix guaranteed to have real eigenvalues.

\subsection{QR Algorithm}
\begin{itemize}
    \item This algorithm iteratively transforms a matrix into another similar matrix by factoring it into a product of an orthogonal matrix $(Q)$ and an upper triangular matrix $(R)$. The original matrix is updated as the product $(RQ)$:
    \begin{align*}
        A_0 &= Q_0R_0, \\
        A_1 &= R_0Q_0, \\
        A_1 &= Q_1R_1, \\
        A_2 &= R_1Q_1, \\
            &\vdots \\
        A_i &= R_{i-1}Q_{i-1}, \\
        A_i &= Q_iR_i.
    \end{align*}
    \item Since $Q$ is orthogonal, $Q^{-1}$ is replaced with $Q^{\top}$. The iterative process becomes:
    \[
    A_{j+1} = Q_j^{\top}A_jQ_j.
    \]
    The eigenvalues of $A_j$ remain the same as those of the original matrix due to similarity.
    \item $Q$ is computed using the Gram-Schmidt process, where the projections of preceding vectors are removed from each column vector. Each column is then normalized so that the norm of the column is 1.
    \item The iteration continues until $A_j$ converges to a triangular matrix, with diagonal entries representing the eigenvalues. The time complexity of this method is $O(k \cdot n^3)$, where $k$ is the number of iterations and $n$ is the matrix size.
\end{itemize}

\subsection{Householder Transformation}
\begin{itemize}
    \item The QR algorithm is particularly efficient for Hessenberg matrices. A Hessenberg matrix has zeros either below (lower Hessenberg) or above (upper Hessenberg) the first sub-diagonal.
    \item For larger matrices, efficiency is improved by using Householder reflections (with $O(n^3)$ complexity) to transform the matrix into a Hessenberg matrix. This significantly reduces the number of iterations in the QR algorithm.
    \item A Householder transformation reflects a vector $\mathbf{v_1}$ to another vector $\mathbf{v_2}$ (both with the same magnitude). If $\mathbf{u}$ is the unit vector along $\mathbf{v_1} - \mathbf{v_2}$, the reflection matrix is:
    \[
    Q = I - 2\mathbf{uu^{T}}.
    \]
    \item $Q$ is symmetric and orthogonal, so $Q^{-1} = Q$.
    \item At each step, $\mathbf{v_1}$ is chosen as a column vector of the matrix, and $\mathbf{v_2}$ is a vector with the first element equal to $||\mathbf{v_1}||$ and the rest set to 0. The resulting $Q$ is extended to the size of the original matrix by adding zeros and ones as needed.
    \item The update is performed iteratively:
    \[
    A_1 = Q A_0 Q, \quad A_0 := A_1.
    \]
    \item This transformation produces a Hessenberg matrix similar to the original matrix.
    \item Once the matrix is in Hessenberg form, the QR algorithm is applied to compute eigenvalues. For large matrices, the sparsity of the Hessenberg form accelerates convergence.
    \item The C code for this algorithm is available at:
    \href{https://github.com/Pratheek39/EE1030/blob/120977af296ad194235d07bdbaafdb19f6b0e856/QR/main.c}{https://github.com/Pratheek39/EE1030/\\blob/main/QR/main.c}
\end{itemize}

\section{Time Complexity Analysis}
The Householder transformation has a time complexity of $O(n^3)$, and the QR algorithm has $O(k \cdot n^3)$ complexity. Combining the two reduces the number of operations per QR iteration and accelerates convergence by minimizing $k$.

\section{Convergence Rate and Space Complexity}
The convergence rate of the QR algorithm is linear. The convergence rate depends on the ratio of the two largest eigenvalues. For eigenvalues:
\[
|\lambda_1| > |\lambda_2| > \dots > |\lambda_n|,
\]
the convergence rate is proportional to $\frac{|\lambda_2|}{|\lambda_1|}$. The space complexity is $O(n^2)$ for both parts of the process.

\section{Comparison with Other Algorithms}
\begin{itemize}
    \item \textbf{Jacobi Method}: Uses Givens rotations to zero out off-diagonal elements. It is efficient for symmetric matrices but slower than the QR algorithm for larger matrices.
    \item \textbf{Power Iteration}: Simple and computationally inexpensive but finds only the largest eigenvalue. The QR algorithm finds all eigenvalues and converges faster.
    \item \textbf{Divide and Conquer Algorithms}: Recursively divide a matrix and merge results. They are efficient for symmetric matrices but complex to implement and have higher space complexity.
    \item Other algorithms, like the Lanczos algorithm, are simpler and cheaper but only compute a subset of eigenvalues.
\end{itemize}

\section{Conclusion}
To compute all eigenvalues of a matrix guaranteed to have real eigenvalues, the QR algorithm combined with the Householder transformation is efficient, stable, and particularly suitable for dense matrices.

\end{document}

